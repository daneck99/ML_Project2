# ML_Project2

## ğŸ§© Reproducibility Notes
All experiments were conducted in a **Kaggle Notebook** environment using the following configuration:

- **Runtime**: Python 3.11, PyTorch 2.3, Transformers 4.46, PEFT 0.12  
- **Hardware**: Single NVIDIA T4 GPU (16GB VRAM)  
- **Execution time**: ~45 minutes for full fine-tuning (Step 3)

To ensure full reproducibility:

1. **Random Seeds**  
   - All random seeds were fixed to `42` for NumPy, PyTorch, and scikit-learn.  
   - Dataset splitting (`train_test_split`) used stratified sampling for consistent class balance.

2. **Offline Environment**  
   - The notebook was executed **without internet access**.  
   - All dependencies and models were preloaded from Kaggle datasets or `/kaggle/input` directories:  
     ```
     /kaggle/input/deberta-v3-small-local
     /kaggle/input/e5-small-v2
     /kaggle/input/llm-classification-finetuning
     ```
   - No online downloads or API calls were required.

3. **Deterministic Workflow**  
   - Cell 1: Define paths, environment variables, and shared arguments (`BASE_ARGS`)  
   - Cell 2â€“3: Load and preprocess CSVs (train/test/submit)  
   - Cell 4: Feature or embedding extraction  
   - Cell 5: Model training (Step 1â€“3)  
   - Cell 6: Inference and submission file generation (`submission.csv`)

4. **Version Control & Dependencies**  
   - All package versions are pinned to prevent dependency drift.  
   - The same results can be reproduced on any Kaggle GPU runtime with the same model folders.

**Note:** No internet connection or external API access is required to reproduce the results.



### Step 1. Import Models and Datasets

Before running the notebook, add all required models and datasets to your Kaggle environment.

#### ğŸ“¦ How to Add
1. Go to the right sidebar in your Kaggle Notebook â†’ **"Add data"**
2. Add the following inputs (either as **Models** or **Datasets**):
   - `microsoft/deberta-v3-small` â†’ (as model, or upload as dataset: `/kaggle/input/deberta-v3-small-local/`)
   - `llm-classification-finetuning` â†’ (dataset with `train.csv`, `test.csv`, `sample_submission.csv`)

> ğŸ’¡ You can find these either under **â€œModelsâ€** tab in Kaggle, or upload them manually as datasets.  
> Make sure **Internet is OFF**, since all weights and data are local.
>
> 

This section documents all details required to reproduce the experiments.

### ğŸ”§ Environment
- **Runtime:** Kaggle GPU (T4 * 2)
- **Core Libraries:**
  - `torch`
  - `transformers`
  - `sentence-transformers`
  - `datasets`
  - `scikit-learn`
  - `pandas`, `numpy`, `matplotlib`, `tqdm`
- **Mixed Precision:** `torch.amp.autocast("cuda")` (automatic mixed precision enabled)

### âš™ï¸ Final Model Training Configuration
| Setting | Value |
|----------|--------|
| `random_state` | 42 |
| `epochs` | 5 |
| `optimizer` | AdamW (lr = 1.2e-5) |
| `scheduler` | Cosine with 6% warmup |
| `label_smoothing` | 0.08 |
| `weight_decay` | 0.03 |
| `dropout` | 0.2 (hidden/classifier) |
| `early_stopping` | patience = 2 |

### ğŸ”’ Handling No-Internet Constraint
All external model weights were **pre-uploaded to `/kaggle/input/`** directories:
