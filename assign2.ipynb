{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPu9Z768IGTxpE8qIttDFsU"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13613695,"sourceType":"datasetVersion","datasetId":8651649}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##   STEP0: 런타임 준비 + 기본 설정\n\n\n","metadata":{"id":"OpJnRNwZ7Uvd"}},{"cell_type":"code","source":"#!pip -q install -U sentence-transformers transformers peft datasets\n\nimport os, sys, json\nimport numpy as np, pandas as pd\nprint(\"Python:\", sys.version)\n\n# 경로/하이퍼파라미터 공통\nDEBERTA_DIR = \"/kaggle/input/deberta-v3-small-local\"\nQWEN3_DIR = \"/kaggle/input/e5-small-v2/transformers/default/1\"  \nDATA_DIR = \"/kaggle/input/llm-classification-finetuning\"  # Colab/로컬이면 적절히 교체\nWORK_DIR = \"/kaggle/working\"\n\nBASE_ARGS = {\n    \"DATA_DIR\": DATA_DIR,\n    \"WORK_DIR\": WORK_DIR,\n    \"train_path\": f\"{DATA_DIR}/train.csv\",\n    \"test_path\":  f\"{DATA_DIR}/test.csv\",\n    \"submit_path\": f\"{DATA_DIR}/sample_submission.csv\",\n    \"embed_model_dir\": QWEN3_DIR,  \n    \"embed_batch_size\": 64,         \n    \"random_state\": 42,\n    \"val_size\": 0.2,\n}\n\n# 파일 존재 확인\nfor k in [\"train_path\",\"test_path\",\"submit_path\"]:\n    print(k, os.path.exists(BASE_ARGS[k]), \"->\", BASE_ARGS[k])\n","metadata":{"id":"88EyYNsJ7Uc7","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:14:33.903700Z","iopub.execute_input":"2025-11-06T08:14:33.904260Z","iopub.status.idle":"2025-11-06T08:14:34.165658Z","shell.execute_reply.started":"2025-11-06T08:14:33.904238Z","shell.execute_reply":"2025-11-06T08:14:34.165032Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\ntrain_path True -> /kaggle/input/llm-classification-finetuning/train.csv\ntest_path True -> /kaggle/input/llm-classification-finetuning/test.csv\nsubmit_path True -> /kaggle/input/llm-classification-finetuning/sample_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## STEP0: Data preprocessing + labeling","metadata":{"id":"ig3HnVFV7jYp"}},{"cell_type":"code","source":"from typing import Dict, Tuple, Optional, List\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\ndef load_csvs(cfg: Dict):\n    train = pd.read_csv(cfg[\"train_path\"])\n    test  = pd.read_csv(cfg[\"test_path\"])\n    submit= pd.read_csv(cfg[\"submit_path\"])\n    print(f\"[io] train:{train.shape}, test:{test.shape}, submit:{submit.shape}\")\n    return train, test, submit\n\ndef standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    low = {c.lower(): c for c in df.columns}\n    if \"response_a\" in low: df = df.rename(columns={low[\"response_a\"]: \"response_A\"})\n    if \"response_b\" in low: df = df.rename(columns={low[\"response_b\"]: \"response_B\"})\n    if \"prompt\" not in df.columns: df[\"prompt\"] = \"\"\n    return df\n    \ndef build_label_threeway(train: pd.DataFrame) -> pd.DataFrame:\n    low = {c.lower(): c for c in train.columns}\n    need = {\"winner_model_a\",\"winner_model_b\",\"winner_tie\"}\n    if not need.issubset(set(low)):\n        raise ValueError(\"3-class 라벨 생성 불가: winner_* 세 컬럼 필요\")\n\n    a = pd.to_numeric(train[low[\"winner_model_a\"]], errors=\"coerce\").fillna(0).astype(int)\n    b = pd.to_numeric(train[low[\"winner_model_b\"]], errors=\"coerce\").fillna(0).astype(int)\n    t = pd.to_numeric(train[low[\"winner_tie\"]],     errors=\"coerce\").fillna(0).astype(int)\n\n    # 각 행에서 하나만 1이라고 가정(여러 개 1인 이상치는 A/B/tie 우선순위로 최대값 선택)\n    label3 = np.argmax(np.stack([a.values, b.values, t.values], axis=1), axis=1).astype(int)\n\n    out = train.copy()\n    out[\"label\"] = label3  # 0:A, 1:B, 2:tie\n    print(\"[pre/3way] shape:\", out.shape,\n          \" label counts:\", dict(pd.Series(label3).value_counts().sort_index()))\n    return out\n\ndef save_submission_threecols(test_df: pd.DataFrame,\n                              probs_3: np.ndarray,  # shape (N, 3) → softmax된 확률\n                              out_path: str):\n    assert probs_3.ndim == 2 and probs_3.shape[1] == 3, \"probs_3 must be (N,3)\"\n    p = np.clip(probs_3, 1e-7, 1-1e-7)\n    p = p / p.sum(axis=1, keepdims=True)  # 혹시 모를 드리프트 교정\n\n    sub = pd.DataFrame({\n        \"id\": test_df[\"id\"].values,\n        \"winner_model_a\": p[:, 0],\n        \"winner_model_b\": p[:, 1],\n        \"winner_tie\":     p[:, 2],\n    })\n\n    # sanity check\n    s = sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].sum(1).to_numpy()\n    assert np.allclose(s, 1.0, atol=1e-6), \"probabilities must sum to 1\"\n\n    sub.to_csv(out_path, index=False)\n    print(\"[save]\", out_path)\n    print(sub.head())","metadata":{"id":"dJWKTLd6726Z","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:14:34.167086Z","iopub.execute_input":"2025-11-06T08:14:34.167393Z","iopub.status.idle":"2025-11-06T08:14:34.783791Z","shell.execute_reply.started":"2025-11-06T08:14:34.167375Z","shell.execute_reply":"2025-11-06T08:14:34.783146Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"##STEP 1:TF-IDF Baseline","metadata":{"id":"459G-OVX7590"}},{"cell_type":"code","source":"from scipy.sparse import hstack, csr_matrix, issparse\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\n\ndef add_length_feats(df):\n    f = pd.DataFrame(index=df.index)\n    f[\"len_prompt\"]=df[\"prompt\"].astype(str).str.len()\n    f[\"len_A\"]=df[\"response_A\"].astype(str).str.len()\n    f[\"len_B\"]=df[\"response_B\"].astype(str).str.len()\n    f[\"diff_len\"]=f[\"len_A\"]-f[\"len_B\"]\n    return f.astype(np.float32)\n\ndef add_bias_feats(df):\n    sA=df[\"response_A\"].astype(str).str.strip()\n    sB=df[\"response_B\"].astype(str).str.strip()\n    f = pd.DataFrame(index=df.index)\n    f[\"starts_with_quote_A\"]=sA.str.startswith('\"').astype(int)\n    f[\"starts_with_quote_B\"]=sB.str.startswith('\"').astype(int)\n    f[\"verbosity_A\"]=sA.str.len()\n    f[\"verbosity_B\"]=sB.str.len()\n    f[\"verbosity_diff\"]=f[\"verbosity_A\"]-f[\"verbosity_B\"]\n    return f.astype(np.float32)\n\ndef run_step1_tfidf(cfg: Dict, max_features=20000, ngram=(1,2), use_logreg=False, out_csv=\"submission_step1.csv\"):\n    train, test, submit = load_csvs(cfg)\n    train = standardize_columns(train); test = standardize_columns(test)\n    train = build_label_threeway(train)\n\n    vec = TfidfVectorizer(max_features=max_features, ngram_range=ngram, dtype=np.float32)\n    vec.fit(pd.concat([train[\"response_A\"], train[\"response_B\"]]).astype(str))\n    XA, XB = vec.transform(train[\"response_A\"]), vec.transform(train[\"response_B\"])\n    XtA, XtB = vec.transform(test[\"response_A\"]), vec.transform(test[\"response_B\"])\n    X, Xt = hstack([XA,XB], format=\"csr\"), hstack([XtA,XtB], format=\"csr\")\n\n    # length/bias features\n    X  = hstack([X,  csr_matrix(add_length_feats(train).values), csr_matrix(add_bias_feats(train).values)], format=\"csr\")\n    Xt = hstack([Xt, csr_matrix(add_length_feats(test).values),  csr_matrix(add_bias_feats(test).values)],  format=\"csr\")\n    y = train[\"label\"].astype(int).values\n\n    # model\n    if use_logreg:\n        clf = LogisticRegression(solver=\"saga\", max_iter=300, n_jobs=-1, verbose=1)\n    else:\n        clf = SGDClassifier(loss=\"log_loss\", max_iter=30, tol=1e-3, early_stopping=True,\n                            n_iter_no_change=3, average=True, random_state=cfg[\"random_state\"])\n    X_tr,X_val,y_tr,y_val = train_test_split(X,y,test_size=cfg[\"val_size\"], stratify=y, random_state=cfg[\"random_state\"])\n    clf.fit(X_tr,y_tr)\n    vpred = clf.predict_proba(X_val)[:,1] if hasattr(clf,\"predict_proba\") else 1/(1+np.exp(-clf.decision_function(X_val)))\n    print(\"[step1] Val LogLoss:\", log_loss(y_val, vpred, labels=[0,1]))\n\n    clf.fit(X,y)\n    test_scores = clf.predict_proba(Xt)[:,1] if hasattr(clf,\"predict_proba\") else 1/(1+np.exp(-clf.decision_function(Xt)))\n    save_submission_threecols(submit, test_scores, os.path.join(cfg[\"WORK_DIR\"], out_csv))\n\n#실행\n#run_step1_tfidf(BASE_ARGS)\n","metadata":{"id":"iJfXlk9j8JyM","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:14:34.784483Z","iopub.execute_input":"2025-11-06T08:14:34.784811Z","iopub.status.idle":"2025-11-06T08:14:34.896953Z","shell.execute_reply.started":"2025-11-06T08:14:34.784793Z","shell.execute_reply":"2025-11-06T08:14:34.896106Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## STEP2","metadata":{"id":"M4XjVxoC8l7X"}},{"cell_type":"code","source":"# =========================\n# Step2: Fast Embedding Model (MiniLM/E5/BGE) + tqdm + 3-class OVR\n# =========================\nimport os, time, numpy as np, pandas as pd, torch\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import log_loss\n\n# 1) 범용 임베더: sentence-transformers 우선, 없으면 HF로 폴백(mean pooling)\nclass FastEmbedder:\n    def __init__(self, model_path_or_name: str,\n                 normalize: bool=True, max_length: int=256,\n                 use_instruction: bool=False):\n        self.normalize = normalize\n        self.max_length = max_length\n        self.use_instruction = use_instruction\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        self.backend = \"st\"\n        try:\n            from sentence_transformers import SentenceTransformer\n            if os.path.exists(model_path_or_name):\n                print(\"[embed] sentence-transformers local:\", model_path_or_name)\n                self.st = SentenceTransformer(model_path_or_name, device=self.device)\n            else:\n                print(\"[embed] sentence-transformers hub:\", model_path_or_name)\n                self.st = SentenceTransformer(model_path_or_name, device=self.device)\n        except Exception as e:\n            print(\"[embed] ST import failed, fallback HF:\", e)\n            self.backend = \"hf\"\n            from transformers import AutoTokenizer, AutoModel\n            self.tok = AutoTokenizer.from_pretrained(model_path_or_name, use_fast=True)\n            self.model = AutoModel.from_pretrained(model_path_or_name).to(self.device).eval()\n\n        # e5/bge instruction prefix 힌트\n        lname = model_path_or_name.lower()\n        self.is_e5  = \"e5\" in lname\n        self.is_bge = \"bge\" in lname\n\n    def _maybe_prefix(self, series, kind):\n        # kind: \"prompt\" or \"response\"\n        if not self.use_instruction:\n            return series.astype(str)\n        s = series.astype(str)\n        if self.is_e5 or self.is_bge:\n            if kind == \"prompt\":\n                return \"query: \" + s\n            else:\n                return \"passage: \" + s\n        return s\n\n    @torch.no_grad()\n    def encode(self, series, batch_size=256, desc=\"Embedding\", kind=\"response\"):\n        texts = self._maybe_prefix(series, kind)\n        arr = texts.astype(str).tolist()\n\n        if self.backend == \"st\":\n            # ST가 내부 배치/FP16 최적화 수행\n            embs = self.st.encode(\n                arr, batch_size=batch_size, show_progress_bar=True,\n                normalize_embeddings=self.normalize\n            )\n            return np.asarray(embs, dtype=np.float32)\n\n        # HF 폴백(mean pooling)\n        from transformers import logging as hf_logging\n        hf_logging.set_verbosity_error()\n        out = []\n        t0 = time.time()\n        for i in tqdm(range(0, len(arr), batch_size), desc=desc, ncols=90):\n            batch = arr[i:i+batch_size]\n            toks = self.tok(\n                batch, padding=True, truncation=True,\n                max_length=self.max_length, return_tensors=\"pt\"\n            ).to(self.device)\n            hidden = self.model(**toks).last_hidden_state  # (B,T,H)\n            mask = toks[\"attention_mask\"].unsqueeze(-1)\n            vecs = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n            if self.normalize:\n                vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n            out.append(vecs.float().cpu().numpy())\n        print(f\"[{desc}] {len(arr)} rows in {time.time()-t0:.1f}s\")\n        return np.vstack(out).astype(np.float32)\n\ndef run_step2_embed_fast(cfg: dict,\n                         model_path_or_name: str,\n                         bs_embed: int = 256,\n                         out_csv: str = \"submission_step2_fast.csv\",\n                         max_length: int = 256,\n                         use_instruction: bool = False):\n    t0 = time.time()\n    # 0) 데이터\n    train, test, _ = load_csvs(cfg)\n    train = standardize_columns(train); test = standardize_columns(test)\n    train = build_label_threeway(train)  # label in {0:A,1:B,2:tie}\n    for c in [\"prompt\",\"response_A\",\"response_B\"]:\n        if c not in train.columns: train[c] = \"\"\n        if c not in test.columns:  test[c]  = \"\"\n\n    # 1) 임베딩\n    emb = FastEmbedder(model_path_or_name, normalize=True,\n                       max_length=max_length, use_instruction=use_instruction)\n    with tqdm(total=6, desc=\"[embed] all\", ncols=90) as pbar:\n        Ep_tr = emb.encode(train[\"prompt\"],     batch_size=bs_embed, desc=\"Train P\", kind=\"prompt\");  pbar.update(1)\n        Ea_tr = emb.encode(train[\"response_A\"], batch_size=bs_embed, desc=\"Train A\", kind=\"response\"); pbar.update(1)\n        Eb_tr = emb.encode(train[\"response_B\"], batch_size=bs_embed, desc=\"Train B\", kind=\"response\"); pbar.update(1)\n        Ep_te = emb.encode(test[\"prompt\"],      batch_size=bs_embed, desc=\"Test  P\", kind=\"prompt\");  pbar.update(1)\n        Ea_te = emb.encode(test[\"response_A\"],  batch_size=bs_embed, desc=\"Test  A\", kind=\"response\"); pbar.update(1)\n        Eb_te = emb.encode(test[\"response_B\"],  batch_size=bs_embed, desc=\"Test  B\", kind=\"response\"); pbar.update(1)\n\n    # 2) 피처 (그대로 재사용)\n    def make_feats(Ep, Ea, Eb, df):\n        d_abs = np.abs(Ea - Eb)\n        d_mul = Ea * Eb\n        la = df[\"response_A\"].astype(str).str.len().to_numpy().reshape(-1,1)\n        lb = df[\"response_B\"].astype(str).str.len().to_numpy().reshape(-1,1)\n        lp = df[\"prompt\"].astype(str).str.len().to_numpy().reshape(-1,1)\n        ldiff = la - lb\n        return np.hstack([d_abs, d_mul, la, lb, lp, ldiff]).astype(np.float32)\n\n    X_tr = make_feats(Ep_tr, Ea_tr, Eb_tr, train)\n    X_te = make_feats(Ep_te, Ea_te, Eb_te, test)\n    y    = train[\"label\"].astype(int).values\n    print(f\"[feat] train {X_tr.shape}, test {X_te.shape}\")\n\n    # 3) 분류기 (멀티클래스 OVR)\n    clf = SGDClassifier(\n        loss=\"log_loss\", max_iter=50, tol=1e-3,\n        early_stopping=True, n_iter_no_change=3,\n        average=True, random_state=cfg[\"random_state\"]\n    )\n    X_tr1, X_val, y_tr, y_val = train_test_split(\n        X_tr, y, test_size=cfg[\"val_size\"], stratify=y, random_state=cfg[\"random_state\"]\n    )\n    clf.fit(X_tr1, y_tr)\n    vpred = clf.predict_proba(X_val)\n    print(f\"[val] logloss={log_loss(y_val, vpred, labels=[0,1,2]):.4f}\")\n\n    clf.fit(X_tr, y)\n    probs_3 = clf.predict_proba(X_te)\n    probs_3 = np.clip(probs_3, 1e-7, 1-1e-7)\n    probs_3 = probs_3 / probs_3.sum(axis=1, keepdims=True)\n\n    sub = pd.DataFrame({\n        \"id\": test[\"id\"].values,\n        \"winner_model_a\": probs_3[:,0],\n        \"winner_model_b\": probs_3[:,1],\n        \"winner_tie\":     probs_3[:,2],\n    })\n    s = sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].sum(1).to_numpy()\n    assert np.allclose(s, 1.0, atol=1e-6)\n    out_path = os.path.join(cfg[\"WORK_DIR\"], out_csv)\n    sub.to_csv(out_path, index=False)\n    print(f\"[save] {out_path}  total={time.time()-t0:.1f}s\")\n    print(sub.head())\n\n#run_step2_embed_fast(BASE_ARGS,model_path_or_name=BASE_ARGS[\"embed_model_dir\"],bs_embed=256,max_length=256,use_instruction=True, out_csv=\"submission.csv\")","metadata":{"id":"UPIwlxJt8nsY","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:14:34.898425Z","iopub.execute_input":"2025-11-06T08:14:34.898698Z","iopub.status.idle":"2025-11-06T08:14:39.240400Z","shell.execute_reply.started":"2025-11-06T08:14:34.898681Z","shell.execute_reply":"2025-11-06T08:14:39.239644Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"##STEP3","metadata":{"id":"R-KxrWuD8rMN"}},{"cell_type":"code","source":"import os, time, torch, transformers\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model\nfrom packaging.version import parse as V\nfrom sklearn.metrics import log_loss as _logloss\n\ndef _build_inputs_pair(df, tok):\n    sep = tok.sep_token or tok.eos_token or \"</s>\"\n    texts = []\n    for p, a, b in zip(df[\"prompt\"], df[\"response_A\"], df[\"response_B\"]):\n        s = f\"[INST] {p} [/INST] {sep} <A> {a} </A> {sep} <B> {b} </B>\"\n        texts.append(s)\n    return texts\n\ndef run_step3_lora_offline(\n    cfg: dict,\n    model_dir: str,\n    out_csv: str = \"submission_step3_lora.csv\",\n    batch_size: int = 8,\n    epochs: int = 4,\n    lr: float = 2e-5,\n    max_len: int = 512,\n):\n    t0_total = time.time()\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"[lora/offline] device={device}, model_dir={model_dir}\")\n\n    # 1) 데이터\n    train, test, submit = load_csvs(cfg)\n    train = standardize_columns(train); test = standardize_columns(test)\n    train = build_label_threeway(train)\n    for col in [\"prompt\",\"response_A\",\"response_B\"]:\n        if col not in train.columns: train[col] = \"\"\n        if col not in test.columns:  test[col]  = \"\"\n\n    # 2) 토크나이저/모델\n    print(\"[init] loading tokenizer/model ...\")\n    tok  = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n    base = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=3)\n    target_modules = [\"query_proj\",\"value_proj\"]\n    lcfg = LoraConfig(r=8, lora_alpha=16, target_modules=target_modules,\n                      lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n    model = get_peft_model(base, lcfg).to(device)\n\n    # 3) 입력 구성\n    print(\"[prep] building input pairs...\")\n    tr_texts = _build_inputs_pair(train, tok)\n    te_texts = _build_inputs_pair(test, tok)\n    labels   = train[\"label\"].astype(int).tolist()\n    \n    ds = Dataset.from_dict({\"text\": tr_texts, \"label\": labels})\n    \n    def preprocess(batch):\n        enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n        enc[\"labels\"] = batch[\"label\"]  # 분류 레이블\n        return enc\n    \n    # batched + tqdm 진행률 + 빠른 배치 사이즈\n    tok_ds = ds.map(\n        preprocess,\n        batched=True,\n        batch_size=1024,\n        remove_columns=[\"text\", \"label\"],\n        desc=\"Tokenizing (batched)\"\n    )\n    \n    split = tok_ds.train_test_split(test_size=cfg[\"val_size\"], seed=cfg[\"random_state\"])\n\n    # 4) TrainingArguments (버전 호환: 오래된 버전은 evaluation_strategy 미지원)\n    import inspect\n    \n    base_kwargs = dict(\n        output_dir=cfg[\"WORK_DIR\"],\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=epochs,\n        learning_rate=lr,\n        logging_dir=f\"{cfg['WORK_DIR']}/logs\",\n        report_to=\"none\",\n    )\n    \n    sig = inspect.signature(TrainingArguments.__init__)\n    params = sig.parameters\n    \n    if \"evaluation_strategy\" in params:\n        # ✅ 최신 스타일 (transformers 4.10+ 등)\n        args_tr = TrainingArguments(\n            **base_kwargs,\n            logging_strategy=\"steps\",\n            logging_steps=50,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_log_loss\",\n            greater_is_better=False, \n            save_total_limit=1,\n        )\n    else:\n        # ✅ 레거시 스타일 (evaluation_strategy 미지원)\n        #   - evaluate_during_training이 있는 구버전용 처리\n        legacy_extra = {}\n        if \"evaluate_during_training\" in params:\n            legacy_extra[\"evaluate_during_training\"] = True\n        if \"logging_steps\" in params:\n            legacy_extra[\"logging_steps\"] = 50\n        if \"save_steps\" in params:\n            legacy_extra[\"save_steps\"] = 500\n        # 일부 구버전에선 load_best_model_at_end/metric_for_best_model도 미지원이므로 생략\n        args_tr = TrainingArguments(\n            **base_kwargs,\n            **legacy_extra\n        )\n        \n    # 5) metrics\n    def compute_metrics(eval_pred):\n        logits, lab = eval_pred\n        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()  # (N,3)\n        return {\"log_loss\": _logloss(lab, probs, labels=[0,1,2])}    # ← labels 지정\n\n    print(\"[train] starting training ...\")\n    trainer = Trainer(\n        model=model,\n        args=args_tr,\n        train_dataset=split[\"train\"],\n        eval_dataset=split[\"test\"],\n        tokenizer=tok,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    print(f\"[train] finished in {time.time()-t0_total:.1f}s\")\n\n    # 6) 추론 (진행률/시간)\n    print(\"[infer] predicting test set ...\")\n    all_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(te_texts), batch_size), desc=\"Inference\", ncols=90):\n            batch = te_texts[i:i+batch_size]\n            te_enc = tok(batch, truncation=True, padding=\"max_length\",\n                         max_length=max_len, return_tensors=\"pt\").to(device)\n            logits = model(**te_enc).logits\n            probs3 = torch.softmax(logits, dim=-1).cpu().numpy()       # (B, 3)\n            all_probs.append(probs3)\n\n    probs_3 = np.vstack(all_probs)  \n    probs_3 = np.clip(probs_3, 1e-7, 1-1e-7)\n    probs_3 = probs_3 / probs_3.sum(axis=1, keepdims=True)     \n\n    # test.csv id 순서에 맞춰 DataFrame 구성\n    submit3 = pd.DataFrame({\n        \"id\": test[\"id\"].values,\n        \"winner_model_a\": probs_3[:, 0],\n        \"winner_model_b\": probs_3[:, 1],\n        \"winner_tie\":     probs_3[:, 2],\n    })\n    sums = submit3[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].sum(1)\n    assert np.allclose(sums, 1.0, atol=1e-6)\n\n    out_path = os.path.join(cfg[\"WORK_DIR\"], out_csv)  # 예: submission.csv\n    submit3.to_csv(out_path, index=False)\n    print(\"[save]\", out_path)\n    print(submit3.head())\n    print(\"sum(min,max) =\", float(sums.min()), float(sums.max()))\n    print(f\"✅ Done in {time.time()-t0_total:.1f}s total\")\n'''\nrun_step3_lora_offline(\n    BASE_ARGS,\n    model_dir=DEBERTA_DIR,              # 위에서 입력한 경로\n    out_csv=\"submission.csv\",\n    batch_size=8, epochs=1, lr=2e-5, max_len=512,\n)'''","metadata":{"id":"9F1KR7KQ8uPI","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:14:39.241170Z","iopub.execute_input":"2025-11-06T08:14:39.241555Z","iopub.status.idle":"2025-11-06T08:15:04.379135Z","shell.execute_reply.started":"2025-11-06T08:14:39.241508Z","shell.execute_reply":"2025-11-06T08:15:04.378401Z"}},"outputs":[{"name":"stderr","text":"2025-11-06 08:14:50.722262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762416890.963109      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762416891.023854      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nrun_step3_lora_offline(\\n    BASE_ARGS,\\n    model_dir=DEBERTA_DIR,              # 위에서 입력한 경로\\n    out_csv=\"submission.csv\",\\n    batch_size=8, epochs=1, lr=2e-5, max_len=512,\\n)'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import os, time, torch, transformers\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import TrainerCallback, TrainerControl, TrainerState\nfrom peft import LoraConfig, get_peft_model\nfrom packaging.version import parse as V\n\n# [FIX: v5 호환 + 추론에 필요한 import 누락 보완]\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom typing import Optional\n\ndef _build_inputs_pair(df, tok):\n    sep = tok.sep_token or tok.eos_token or \"</s>\"\n    texts = []\n    for p, a, b in zip(df[\"prompt\"], df[\"response_A\"], df[\"response_B\"]):\n        s = f\"[INST] {p} [/INST] {sep} <A> {a} </A> {sep} <B> {b} </B>\"\n        texts.append(s)\n    return texts\n\ndef _reinit_classification_head(model: torch.nn.Module):\n    # (동일) 분류헤드 재초기화\n    head = None\n    if hasattr(model, \"classifier\") and isinstance(model.classifier, torch.nn.Linear):\n        head = model.classifier\n    elif hasattr(model, \"score\") and isinstance(model.score, torch.nn.Linear):\n        head = model.score\n    elif hasattr(model, \"base_model\") and hasattr(model.base_model, \"classifier\") and isinstance(model.base_model.classifier, torch.nn.Linear):\n        head = model.base_model.classifier\n    if head is not None:\n        torch.nn.init.xavier_uniform_(head.weight)\n        if head.bias is not None:\n            torch.nn.init.zeros_(head.bias)\n\nclass SimpleEarlyStopper(TrainerCallback):\n    def __init__(self, metric_key=\"eval_log_loss\", patience=2, minimize=True, eps=0.0):\n        self.metric_key, self.patience, self.minimize, self.eps = metric_key, patience, minimize, eps\n        self.best = None\n        self.bad = 0\n    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kw):\n        m = kw.get(\"metrics\", {})\n        if self.metric_key not in m: return control\n        curr = m[self.metric_key]\n        if self.best is None:\n            self.best, self.bad = curr, 0\n        else:\n            improved = (curr < self.best - self.eps) if self.minimize else (curr > self.best + self.eps)\n            if improved: self.best, self.bad = curr, 0\n            else:\n                self.bad += 1\n                if self.bad >= self.patience:\n                    control.should_training_stop = True\n        return control\n\ndef run_step3_lora_offline(\n    cfg: dict,\n    model_dir: str,\n    out_csv: str = \"submission_step3_lora.csv\",\n    batch_size: int = 8,\n    epochs: int = 4,\n    lr: float = 2e-5,\n    max_len: int = 512,\n    # 안정화 하이퍼파라미터(기존)\n    warmup_ratio: float = 0.1,\n    weight_decay: float = 0.01,\n    max_grad_norm: float = 1.0,\n    label_smoothing: float = 0.1,\n    early_stopping_patience: int = 2,\n    seed: int = 42,\n    scheduler_type: str = \"cosine\",\n):\n    t0_total = time.time()\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"[lora/offline] device={device}, model_dir={model_dir}\")\n\n    # 시드 및 matmul precision\n    try:\n        torch.manual_seed(seed); np.random.seed(seed)\n        if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n        torch.set_float32_matmul_precision(\"medium\")\n    except Exception as e:\n        print(\"[warn] seed/matmul:\", e)\n\n    # 1) 데이터\n    train, test, submit = load_csvs(cfg)\n    train = standardize_columns(train); test = standardize_columns(test)\n    train = build_label_threeway(train)\n    for col in [\"prompt\",\"response_A\",\"response_B\"]:\n        if col not in train.columns: train[col] = \"\"\n        if col not in test.columns:  test[col]  = \"\"\n\n    # 2) 토크나이저/모델\n    print(\"[init] loading tokenizer/model ...\")\n    tok  = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n    base = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=3)\n    _reinit_classification_head(base)  # 분류헤드 재초기화\n\n    target_modules = [\"query_proj\",\"value_proj\"]\n    lcfg = LoraConfig(r=8, lora_alpha=16, target_modules=target_modules,\n                      lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n    model = get_peft_model(base, lcfg).to(device)\n    try: model.print_trainable_parameters()\n    except: pass\n\n    # 3) 입력 구성\n    print(\"[prep] building input pairs...\")\n    tr_texts = _build_inputs_pair(train, tok)\n    te_texts = _build_inputs_pair(test, tok)\n    labels   = train[\"label\"].astype(int).tolist()\n    ds = Dataset.from_dict({\"text\": tr_texts, \"label\": labels})\n    def preprocess(batch):\n        enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n        enc[\"labels\"] = batch[\"label\"]\n        return enc\n    tok_ds = ds.map(preprocess, batched=True, batch_size=1024, remove_columns=[\"text\",\"label\"],\n                    desc=\"Tokenizing (batched)\")\n    split = tok_ds.train_test_split(test_size=cfg[\"val_size\"], seed=cfg[\"random_state\"])\n\n    # 4) TrainingArguments (버전 호환 강화)\n    sig = inspect.signature(TrainingArguments.__init__)\n    params = sig.parameters\n    def _has_arg(n: str) -> bool: return n in params\n\n    base_kwargs = dict(\n        output_dir=cfg[\"WORK_DIR\"],\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=epochs,\n        learning_rate=lr,\n        logging_dir=f\"{cfg['WORK_DIR']}/logs\",\n        report_to=\"none\",\n        seed=seed,\n        remove_unused_columns=False,\n    )\n\n    stable_kwargs = {}\n    if _has_arg(\"lr_scheduler_type\"):     stable_kwargs[\"lr_scheduler_type\"] = scheduler_type\n    if _has_arg(\"warmup_ratio\"):          stable_kwargs[\"warmup_ratio\"] = warmup_ratio\n    if _has_arg(\"weight_decay\"):          stable_kwargs[\"weight_decay\"] = weight_decay\n    if _has_arg(\"max_grad_norm\"):         stable_kwargs[\"max_grad_norm\"] = max_grad_norm\n    if _has_arg(\"label_smoothing_factor\"):stable_kwargs[\"label_smoothing_factor\"] = label_smoothing\n    if torch.cuda.is_available():\n        if _has_arg(\"bf16\") and torch.cuda.get_device_capability(0)[0] >= 8:\n            stable_kwargs[\"bf16\"] = True\n        elif _has_arg(\"fp16\"):\n            stable_kwargs[\"fp16\"] = True\n\n    # [FIX: 얼리 스톱핑이 요구하는 필드 지원 여부 개별 점검]\n    can_load_best = _has_arg(\"load_best_model_at_end\")\n    can_metric    = _has_arg(\"metric_for_best_model\")\n    can_greater   = _has_arg(\"greater_is_better\")\n\n    extra_eval_kwargs = {}\n    if \"evaluation_strategy\" in params:\n        extra_eval_kwargs[\"evaluation_strategy\"] = \"epoch\"\n        if _has_arg(\"save_strategy\"): extra_eval_kwargs[\"save_strategy\"] = \"epoch\"\n        if can_load_best: extra_eval_kwargs[\"load_best_model_at_end\"] = True  # 요구됨\n        if can_metric:    extra_eval_kwargs[\"metric_for_best_model\"] = \"eval_log_loss\"\n        if can_greater:   extra_eval_kwargs[\"greater_is_better\"] = False\n        if _has_arg(\"save_total_limit\"): extra_eval_kwargs[\"save_total_limit\"] = 1\n        if _has_arg(\"logging_strategy\"): extra_eval_kwargs[\"logging_strategy\"] = \"steps\"\n        if _has_arg(\"logging_steps\"):    extra_eval_kwargs[\"logging_steps\"] = 50\n        if _has_arg(\"optim\"):            extra_eval_kwargs[\"optim\"] = \"adamw_torch\"\n    else:\n        # 레거시 대응\n        if _has_arg(\"evaluate_during_training\"): extra_eval_kwargs[\"evaluate_during_training\"] = True\n        if _has_arg(\"logging_steps\"):            extra_eval_kwargs[\"logging_steps\"] = 50\n        if _has_arg(\"save_steps\"):               extra_eval_kwargs[\"save_steps\"] = 500\n        # 이 경우 load_best/metric 지원이 없을 수 있으므로 EarlyStopping 비활성화 예정\n\n    args_tr = TrainingArguments(**base_kwargs, **stable_kwargs, **extra_eval_kwargs)\n\n    # 5) metrics\n    def compute_metrics(eval_pred):\n        logits, lab = eval_pred\n        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n        return {\"log_loss\": _logloss(lab, probs, labels=[0,1,2])}\n\n    # [FIX] Trainer 시그니처를 확인해서 **지원되는 인자만** 전달\n    trainer_sig_keys = set(inspect.signature(Trainer.__init__).parameters.keys())\n\n    trainer_kwargs = {\n        \"model\": model,\n        \"args\": args_tr,\n        \"train_dataset\": split[\"train\"],\n        \"eval_dataset\": split[\"test\"],\n        \"compute_metrics\": compute_metrics,\n    }\n\n    # [FIX] v5 권장: processing_class가 있으면 사용, 아니면 tokenizer 사용\n    if \"processing_class\" in trainer_sig_keys:\n        trainer_kwargs[\"processing_class\"] = tok   # [FIX]\n    elif \"tokenizer\" in trainer_sig_keys:\n        trainer_kwargs[\"tokenizer\"] = tok          # [fallback]\n\n    # [FIX] label_names는 버전에 따라 __init__ 인자가 아님 → 있으면만 넣기\n    if \"label_names\" in trainer_sig_keys:\n        trainer_kwargs[\"label_names\"] = [\"labels\"]  # [FIX] Peft 경고 방지\n\n    # [FIX] 콜백은 지원할 때만\n    callbacks = []\n    try:\n        from transformers import EarlyStoppingCallback\n        if getattr(args_tr, \"metric_for_best_model\", None) and getattr(args_tr, \"load_best_model_at_end\", False):\n            callbacks.append(EarlyStoppingCallback(\n                early_stopping_patience=early_stopping_patience,\n                early_stopping_threshold=0.0\n            ))\n        else:\n            print(\"[info] EarlyStoppingCallback skipped (metric_for_best_model/load_best_model_at_end 미지원/비활성).\")\n    except Exception as e:\n        print(\"[warn] EarlyStoppingCallback not available:\", e)\n\n    if callbacks and \"callbacks\" in trainer_sig_keys:\n        trainer_kwargs[\"callbacks\"] = callbacks     # [FIX]\n\n    print(\"[train] starting training ...\")\n    trainer = Trainer(**trainer_kwargs)\n    trainer.add_callback(SimpleEarlyStopper(metric_key=\"eval_log_loss\", patience=2, minimize=True))\n\n    # [FIX] __init__에서 label_names 인자를 못 받는 버전 대비:\n    #       속성이 있으면 직접 주입해서 경고 억제 (없으면 무시)\n    if hasattr(trainer, \"label_names\") and not getattr(trainer, \"label_names\"):\n        trainer.label_names = [\"labels\"]            # [FIX]\n\n    trainer.train()\n    print(f\"[train] finished in {time.time()-t0_total:.1f}s\")\n\n    # 6) 추론\n    print(\"[infer] predicting test set ...\")\n    model.eval()\n    all_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(te_texts), batch_size), desc=\"Inference\", ncols=90):\n            batch = te_texts[i:i+batch_size]\n            te_enc = tok(batch, truncation=True, padding=\"max_length\",\n                         max_length=max_len, return_tensors=\"pt\").to(device)\n            logits = model(**te_enc).logits\n            probs3 = torch.softmax(logits, dim=-1).cpu().numpy()\n            all_probs.append(probs3)\n\n    probs_3 = np.vstack(all_probs)\n    probs_3 = np.clip(probs_3, 1e-7, 1-1e-7)\n    probs_3 = probs_3 / probs_3.sum(axis=1, keepdims=True)\n\n    submit3 = pd.DataFrame({\n        \"id\": test[\"id\"].values,\n        \"winner_model_a\": probs_3[:, 0],\n        \"winner_model_b\": probs_3[:, 1],\n        \"winner_tie\":     probs_3[:, 2],\n    })\n    sums = submit3[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].sum(1)\n    assert np.allclose(sums, 1.0, atol=1e-6)\n\n    out_path = os.path.join(cfg[\"WORK_DIR\"], out_csv)\n    submit3.to_csv(out_path, index=False)\n    print(\"[save]\", out_path)\n    print(submit3.head())\n    print(\"sum(min,max) =\", float(sums.min()), float(sums.max()))\n    print(f\"✅ Done in {time.time()-t0_total:.1f}s total\")\n'''\nrun_step3_lora_offline(\n    BASE_ARGS,\n    model_dir=DEBERTA_DIR,\n    out_csv=\"submission.csv\",\n    batch_size=8,\n    epochs=6,\n    lr=2e-5,\n    max_len=512,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    label_smoothing=0.02,\n    early_stopping_patience=2,\n    scheduler_type=\"cosine\",\n)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:15:04.380046Z","iopub.execute_input":"2025-11-06T08:15:04.380644Z","iopub.status.idle":"2025-11-06T08:15:04.415359Z","shell.execute_reply.started":"2025-11-06T08:15:04.380626Z","shell.execute_reply":"2025-11-06T08:15:04.414796Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\nrun_step3_lora_offline(\\n    BASE_ARGS,\\n    model_dir=DEBERTA_DIR,\\n    out_csv=\"submission.csv\",\\n    batch_size=8,\\n    epochs=6,\\n    lr=2e-5,\\n    max_len=512,\\n    warmup_ratio=0.1,\\n    weight_decay=0.01,\\n    max_grad_norm=1.0,\\n    label_smoothing=0.02,\\n    early_stopping_patience=2,\\n    scheduler_type=\"cosine\",\\n)\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os, sys, json, math, random, time\nimport numpy as np, pandas as pd\nfrom typing import Dict, Tuple, Optional, List\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n)\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\n\n\n# ==== Utils ====\ndef seed_everything(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\ndef count_trainable_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# ==== Dataset ====\nclass Pairwise3ClsDataset(Dataset):\n    \"\"\"\n    입력: prompt, response_A, response_B → 3-class(0:A, 1:B, 2:tie)\n    텍스트 포맷: [CLS] prompt [SEP] A [SEP] B\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int = 384, has_label: bool = True):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.has_label = has_label\n\n        # 컬럼 이름 표준화(안전)\n        self.df = standardize_columns(self.df)\n\n        need_cols = [\"response_A\", \"response_B\", \"prompt\"]\n        for c in need_cols:\n            if c not in self.df.columns:\n                raise ValueError(f\"Missing column: {c}\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        prompt = str(row.get(\"prompt\", \"\")) or \"\"\n        a = str(row[\"response_A\"])\n        b = str(row[\"response_B\"])\n\n        # 텍스트 구성\n        # DeBERTa도 sep_token 사용 가능\n        sep = self.tokenizer.sep_token or \"[SEP]\"\n        text = f\"{prompt} {sep} [Response A] {a} {sep} [Response B] {b}\"\n\n        item = {\"text\": text}\n        if self.has_label:\n            label = int(row[\"label\"])\n            item[\"label\"] = label\n        else:\n            item[\"id\"] = row[\"id\"]\n        return item\n\ndef collate_fn_train(batch, tokenizer, max_len):\n    texts = [b[\"text\"] for b in batch]\n    enc = tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return enc, labels\n\ndef collate_fn_test(batch, tokenizer, max_len):\n    texts = [b[\"text\"] for b in batch]\n    ids = [b[\"id\"] for b in batch]\n    enc = tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    return enc, ids\n\n# ==== Training / Eval Routines ====\ndef get_optimizer(model, lr=2e-5, weight_decay=0.01):\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n    param_groups = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    return torch.optim.AdamW(param_groups, lr=lr)\n\n@torch.no_grad()\ndef evaluate(model, val_loader, device, label_smoothing=0.0, return_preds=False):\n    model.eval()\n    ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction=\"mean\")\n    all_probs, all_labels = [], []\n    total_loss = 0.0\n\n    for enc, labels in val_loader:\n        for k in enc:\n            enc[k] = enc[k].to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        logits = model(**enc).logits\n        loss = ce(logits, labels)\n        total_loss += loss.item() * labels.size(0)\n\n        probs = torch.softmax(logits, dim=-1).cpu().numpy()\n        all_probs.append(probs)\n        all_labels.append(labels.cpu().numpy())\n\n    all_probs = np.concatenate(all_probs, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    preds = np.argmax(all_probs, axis=1)\n\n    val_logloss = log_loss(all_labels, all_probs, labels=[0,1,2])\n    avg_loss = total_loss / len(val_loader.dataset)\n\n    if return_preds:\n        return avg_loss, val_logloss, all_probs, preds, all_labels\n    return avg_loss, val_logloss, all_probs\n\ndef train_one_epoch(model, train_loader, optimizer, scheduler, device, scaler, grad_accum_steps=1, max_grad_norm=1.0, label_smoothing=0.0):\n    model.train()\n    ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction=\"mean\")\n\n    running_loss = 0.0\n    optimizer.zero_grad(set_to_none=True)\n\n    for step, (enc, labels) in enumerate(train_loader):\n        for k in enc:\n            enc[k] = enc[k].to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n\n        with torch.cuda.amp.autocast(enabled=scaler is not None):\n            outputs = model(**enc)\n            logits = outputs.logits\n            loss = ce(logits, labels) / grad_accum_steps\n\n        if scaler is not None:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n\n        if (step + 1) % grad_accum_steps == 0:\n            if scaler is not None:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n                optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            if scheduler is not None:\n                scheduler.step()\n\n        running_loss += loss.item() * grad_accum_steps\n\n    return running_loss / len(train_loader)\n\n# ==== Main Runner ====\ndef run_full_finetune(cfg: Dict):\n    seed_everything(cfg.get(\"random_state\", 42))\n\n    # I/O\n    train_df, test_df, submit_df = load_csvs(cfg)\n    train_df = standardize_columns(train_df)\n    test_df  = standardize_columns(test_df)\n\n    # 라벨 생성(0:A, 1:B, 2:tie)\n    train_df = build_label_threeway(train_df)\n\n    # Split\n    trn, val = train_test_split(\n        train_df,\n        test_size=cfg.get(\"val_size\", 0.2),\n        random_state=cfg.get(\"random_state\", 42),\n        stratify=train_df[\"label\"]\n    )\n    trn = trn.reset_index(drop=True)\n    val = val.reset_index(drop=True)\n\n    # Tokenizer / Model\n    model_name_or_path = cfg.get(\"model_dir\", DEBERTA_DIR)\n    num_labels = 3\n    max_len = cfg.get(\"max_len\", 384)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n    config = AutoConfig.from_pretrained(\n        model_name_or_path,\n        num_labels=num_labels,\n        problem_type=\"single_label_classification\",\n        hidden_dropout_prob=0.2,          \n        attention_probs_dropout_prob=0.1, \n        classifier_dropout=0.2            \n    )\n    model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=config)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n\n    # Dataset / Loader\n    train_ds = Pairwise3ClsDataset(trn, tokenizer, max_len=max_len, has_label=True)\n    val_ds   = Pairwise3ClsDataset(val, tokenizer, max_len=max_len, has_label=True)\n    test_ds  = Pairwise3ClsDataset(test_df, tokenizer, max_len=max_len, has_label=False)\n\n    train_bs = cfg.get(\"train_batch_size\", 8)\n    val_bs   = cfg.get(\"valid_batch_size\", 16)\n    test_bs  = cfg.get(\"test_batch_size\", 16)\n\n    train_loader = DataLoader(train_ds, batch_size=train_bs, shuffle=True, num_workers=2,\n                              pin_memory=True, collate_fn=lambda b: collate_fn_train(b, tokenizer, max_len))\n    val_loader   = DataLoader(val_ds, batch_size=val_bs, shuffle=False, num_workers=2,\n                              pin_memory=True, collate_fn=lambda b: collate_fn_train(b, tokenizer, max_len))\n    test_loader  = DataLoader(test_ds, batch_size=test_bs, shuffle=False, num_workers=2,\n                              pin_memory=True, collate_fn=lambda b: collate_fn_test(b, tokenizer, max_len))\n\n    # Optim / Sched\n    lr = cfg.get(\"lr\", 2e-5)\n    epochs = cfg.get(\"epochs\", 5)\n    grad_accum = cfg.get(\"grad_accum_steps\", 1)\n    warmup_ratio = cfg.get(\"warmup_ratio\", 0.1)\n    max_grad_norm = cfg.get(\"max_grad_norm\", 1.0)\n    label_smoothing = cfg.get(\"label_smoothing\", 0.05)\n\n    total_steps = math.ceil(len(train_loader) / grad_accum) * epochs\n    warmup_steps = int(total_steps * warmup_ratio)\n\n    optimizer = get_optimizer(model, lr=lr, weight_decay=cfg.get(\"weight_decay\", 0.01))\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\n    # AMP\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n\n    # Early Stopping & Checkpoint\n    patience = cfg.get(\"patience\", 2)\n    best_val_logloss = 1e9\n    best_path = os.path.join(cfg[\"WORK_DIR\"], \"deberta3_small_fullFT_best.pt\")\n    no_improve = 0\n\n    print(f\"[init] Trainable params: {count_trainable_params(model):,}\")\n    print(f\"[sched] total_steps={total_steps}, warmup_steps={warmup_steps}\")\n    print(f\"[hp] epochs={epochs}, bs={train_bs}, accum={grad_accum}, lr={lr}, label_smoothing={label_smoothing}\")\n\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss = train_one_epoch(\n            model, train_loader, optimizer, scheduler, device, scaler,\n            grad_accum_steps=grad_accum, max_grad_norm=max_grad_norm, label_smoothing=label_smoothing\n        )\n        val_loss, val_logloss, _ = evaluate(model, val_loader, device, label_smoothing=label_smoothing)\n        dt = time.time() - t0\n        print(f\"[epoch {ep}/{epochs}] train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}  val_logloss={val_logloss:.5f}  ({dt:.1f}s)\")\n\n        if val_logloss < best_val_logloss - 1e-6:\n            best_val_logloss = val_logloss\n            torch.save({\"model_state\": model.state_dict(), \"config\": config.to_dict()}, best_path)\n            print(f\"  -> New best! saved: {best_path}\")\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(f\"  -> Early stopping (patience={patience})\")\n                break\n\n    # Load best\n    if os.path.exists(best_path):\n        state = torch.load(best_path, map_location=\"cpu\")\n        model.load_state_dict(state[\"model_state\"])\n        model.to(device)\n        print(f\"[load] best checkpoint (val_logloss={best_val_logloss:.5f})\")\n\n        # ---------- Validation analysis: Confusion Matrix & Misclassified ----------\n    val_loss, val_logloss, val_probs, val_preds, val_labels = evaluate(\n        model, val_loader, device, label_smoothing=label_smoothing, return_preds=True\n    )\n    label_names = [\"A win\",\"B win\",\"Tie\"]\n\n    # Confusion Matrix (표시 + 저장)\n    cm = confusion_matrix(val_labels, val_preds, labels=[0,1,2])\n\n    plt.figure(figsize=(5.5, 4.5))\n    plt.imshow(cm, interpolation=\"nearest\")\n    plt.title(\"Confusion Matrix (Validation)\")\n    plt.colorbar()\n    tick_marks = np.arange(len(label_names))\n    plt.xticks(tick_marks, label_names, rotation=0)\n    plt.yticks(tick_marks, label_names)\n    # 숫자 표시\n    thresh = cm.max() / 2.0 if cm.size else 0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], \"d\"),\n                     ha=\"center\", va=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel(\"True\")\n    plt.xlabel(\"Predicted\")\n    plt.tight_layout()\n    cm_path = os.path.join(cfg[\"WORK_DIR\"], \"confusion_matrix_val.png\")\n    plt.savefig(cm_path, dpi=150)\n    plt.show()\n    print(f\"[viz] confusion matrix saved to: {cm_path}\")\n\n    # Classification Report (콘솔)\n    print(classification_report(val_labels, val_preds, target_names=label_names, digits=4))\n\n    # Misclassified 샘플 정리(+ 저장)\n    wrong_idx = np.where(val_preds != val_labels)[0]\n    print(f\"[analysis] misclassified: {len(wrong_idx)} / {len(val_labels)}\")\n\n    # val 데이터프레임에서 해당 행 매칭\n    # (val_loader 순서 = val.reset_index(drop=True) 기반이므로 인덱스 align)\n    val_reset = val.reset_index(drop=True).copy()\n    mis_df = val_reset.iloc[wrong_idx].copy()\n    mis_df[\"true_label\"] = [label_names[i] for i in val_labels[wrong_idx]]\n    mis_df[\"pred_label\"] = [label_names[i] for i in val_preds[wrong_idx]]\n    mis_df[\"p_A\"] = val_probs[wrong_idx, 0]\n    mis_df[\"p_B\"] = val_probs[wrong_idx, 1]\n    mis_df[\"p_Tie\"] = val_probs[wrong_idx, 2]\n\n    mis_path = os.path.join(cfg[\"WORK_DIR\"], \"misclassified_val.csv\")\n    cols = [\"prompt\",\"response_A\",\"response_B\",\"true_label\",\"pred_label\",\"p_A\",\"p_B\",\"p_Tie\"]\n    # 존재하는 컬럼만 저장(안전)\n    cols = [c for c in cols if c in mis_df.columns]\n    mis_df.to_csv(mis_path, index=False, columns=cols)\n    print(f\"[save] misclassified samples saved to: {mis_path}\")\n\n    # 콘솔에 샘플 몇 개만 보기\n    show_n = min(5, len(wrong_idx))\n    for i in range(show_n):\n        ridx = wrong_idx[i]\n        row = val_reset.iloc[ridx]\n        print(\"=\"*80)\n        print(f\"[Prompt]\\n{row.get('prompt','')}\\n\")\n        print(f\"[Response A]\\n{row.get('response_A','')[:500]}...\\n\")\n        print(f\"[Response B]\\n{row.get('response_B','')[:500]}...\\n\")\n        print(f\"✅ True: {label_names[val_labels[ridx]]} | ❌ Pred: {label_names[val_preds[ridx]]} | \"\n              f\"p=[A:{val_probs[ridx,0]:.3f}, B:{val_probs[ridx,1]:.3f}, Tie:{val_probs[ridx,2]:.3f}]\")\n\n    # Inference on test\n    model.eval()\n    all_probs = []\n    all_ids = []\n    with torch.no_grad():\n        for enc, ids in test_loader:\n            for k in enc:\n                enc[k] = enc[k].to(device, non_blocking=True)\n            logits = model(**enc).logits\n            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n            all_probs.append(probs)\n            all_ids.extend(ids)\n    all_probs = np.concatenate(all_probs, axis=0)\n\n    # Save submission\n    out_csv = os.path.join(cfg[\"WORK_DIR\"], \"submission.csv\")\n    test_df_sorted = test_df.copy()\n    # test_df에 id가 반드시 있다고 가정\n    # 혹시 정렬이 섞였을 수 있으니 all_ids 순서대로 데이터프레임 맞추기\n    test_df_sorted = test_df.set_index(\"id\").loc[all_ids].reset_index()\n    save_submission_threecols(test_df_sorted, all_probs, out_csv)\n    print(\"[done]\")\n\n# ==== 실행 ====\nCFG = {\n    **BASE_ARGS,\n    # 모델/학습 하이퍼파라미터\n    \"model_dir\": DEBERTA_DIR,\n    \"max_len\": 384,\n    \"train_batch_size\": 8,     # VRAM 여유에 따라 8~16 권장\n    \"valid_batch_size\": 16,\n    \"test_batch_size\": 16,\n    \"epochs\": 5,               # 4~6 범위 권장\n    \"lr\": 1.2e-5,                # 1e-5 ~ 3e-5 사이 권장\n    \"weight_decay\": 0.03,\n    \"grad_accum_steps\": 1,     # VRAM 모자라면 2~4로\n    \"warmup_ratio\": 0.06,\n    \"max_grad_norm\": 1.0,\n    \"label_smoothing\": 0.08,   # 0.0~0.1 사이에서 조절\n    \"patience\": 2,             # early stopping\n}\n\nrun_full_finetune(CFG)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:15:04.416470Z","iopub.execute_input":"2025-11-06T08:15:04.416739Z"}},"outputs":[{"name":"stdout","text":"[io] train:(57477, 9), test:(3, 4), submit:(3, 4)\n[pre/3way] shape: (57477, 10)  label counts: {0: 20064, 1: 19652, 2: 17761}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-small-local and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[init] Trainable params: 141,897,219\n[sched] total_steps=28740, warmup_steps=1724\n[hp] epochs=5, bs=8, accum=1, lr=1.2e-05, label_smoothing=0.08\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/800329554.py:251: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n/tmp/ipykernel_37/800329554.py:152: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=scaler is not None):\n","output_type":"stream"},{"name":"stdout","text":"[epoch 1/5] train_loss=1.0945  val_loss=1.0842  val_logloss=1.08162  (1370.6s)\n  -> New best! saved: /kaggle/working/deberta3_small_fullFT_best.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/800329554.py:152: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=scaler is not None):\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 시각화\nplt.figure(figsize=(7.5, 5))\nplt.plot(history[\"epoch\"], history[\"train_loss\"], marker=\"o\", label=\"train_loss\")\nplt.plot(history[\"epoch\"], history[\"val_loss\"],   marker=\"o\", label=\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.title(\"Train vs Val Loss\")\nplt.grid(True, alpha=0.3)\nplt.legend()\n# 저장\nplt.savefig(os.path.join(WORK_DIR, \"loss_curve.png\"), dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n# (옵션) 학습률 변화 확인\nplt.figure(figsize=(7.5, 4))\nplt.plot(history[\"epoch\"], history[\"lr_last\"], marker=\"o\", label=\"last_lr\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")\nplt.title(\"LR per epoch (last step)\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}